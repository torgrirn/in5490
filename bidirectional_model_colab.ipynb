{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/kmsravindra/ML-AI-experiments/blob/master/AI/Neural%20Machine%20Translation/Neural%20machine%20translation%20-%20Encoder-Decoder%20seq2seq%20model.ipynb\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload training inputs\n",
    "uploaded_file = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload training targets\n",
    "uploaded_file2 = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training inputs 48\n",
      "Number of training targets 48\n",
      "Input example: [129 129 129 129 129 129 129 129 129 129 129 129  72 129 129 129  71 129\n",
      " 129 129  69 129 129 129  67 129 129 129  67 129 129 129  69 129 129 129\n",
      "  71 129 129 129  72 129 129 129  74 129 129 129  72 129 129 129  71 129\n",
      " 129 129  69 129 129 129  71 129  72 129  69 129 129 129 129 129 129 129\n",
      "  67 129 129 129  72 129 129 129  71 129 129 129  72 129 129 129  74 129\n",
      " 129 129  74 129 129 129  76 129 129 129  74 129  72 129  74 129 129 129\n",
      "  67 129 129 129  69 129 129 129  71 129 129 129  72 129  74 129  76 129\n",
      "  77 129]\n",
      "Target example: [129 129 129 129 129 129 129 129 129 129 129 129  67 129 129 129  67 129\n",
      " 129 129  66 129 129 129  62 129 129 129  64 129  62 129  60 129 129 129\n",
      "  62 129 129 129  64 129 129 129  67 129 129 129 129 129  66 129  67 129\n",
      " 129 129  66 129 129 129  67 129 129 129 129 129 129 129  66 129 129 129\n",
      "  62 129 129 129  67 129 129 129  65 129 129 129  67 129 129 129  67 129\n",
      " 129 129  67 129 129 129  67 129 129 129  66 129 129 129  67 129 129 129\n",
      "  67 129 129 129  62 129 129 129  62 129 129 129  60 129 129 129 129 129\n",
      "  69 129]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and create pairs of inputs and targets\n",
    "\n",
    "with np.load('split_inputs.npz') as split_inputs:\n",
    "    inputs = split_inputs['train']\n",
    "    \n",
    "with np.load('split_targets.npz') as split_targets:\n",
    "    targets = split_targets['train']\n",
    "\n",
    "print(\"Number of training inputs\",len(inputs))\n",
    "print(\"Number of training targets\",len(targets))\n",
    "\n",
    "# Print examples of inputs and targets\n",
    "example_in = inputs[0]\n",
    "example_target = targets[0]\n",
    "print(\"Input example:\", example_in)\n",
    "print(\"Target example:\", example_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set of input notes:\n",
      " [53, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 129]\n",
      "Set of target notes:\n",
      " [43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 129, 200, 201]\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "input_sequences = list(inputs)\n",
    "target_sequences = []\n",
    "input_set = set()\n",
    "target_set = set()\n",
    "num_samples = len(inputs)\n",
    "    \n",
    "for i in range(num_samples):    \n",
    "    target_with_tokens = [200] + list(targets[i]) + [201] # use 200 as start token and 201 as end token\n",
    "    target_sequences.append(target_with_tokens)\n",
    "    \n",
    "    # Add unique notes to set of notes\n",
    "    for item in input_sequences[i]:\n",
    "        if (item not in input_set):\n",
    "            input_set.add(item)\n",
    "    \n",
    "    for item in target_with_tokens:\n",
    "        if (item not in target_set):\n",
    "            target_set.add(item)\n",
    "            \n",
    "input_set = sorted(list(input_set))\n",
    "target_set = sorted(list(target_set))\n",
    "print(\"Set of input notes:\\n\", input_set)\n",
    "print(\"Set of target notes:\\n\", target_set)\n",
    "print(len(input_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to index each input note - key is index and value is the note\n",
    "input_index_to_note_dict = {}\n",
    "\n",
    "# dictionary to get note given its index - key is the note and value is the index\n",
    "input_note_to_index_dict = {}\n",
    "\n",
    "for k, v in enumerate(input_set):\n",
    "    input_index_to_note_dict[k] = v\n",
    "    input_note_to_index_dict[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to index each target note - key is index and value is the note\n",
    "target_index_to_note_dict = {}\n",
    "\n",
    "# dictionary to get note given its index - key is the note and value is the index\n",
    "target_note_to_index_dict = {}\n",
    "\n",
    "for k, v in enumerate(target_set):\n",
    "    target_index_to_note_dict[k] = v\n",
    "    target_note_to_index_dict[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "130\n"
     ]
    }
   ],
   "source": [
    "max_len_inputs = max([len(seq) for seq in input_sequences])\n",
    "max_len_targets = max([len(line) for line in target_sequences])\n",
    "print(max_len_inputs)\n",
    "print(max_len_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input_sequences = np.zeros(shape=(num_samples,max_len_inputs,len(input_set)), dtype='float32')\n",
    "tokenized_target_sequences = np.zeros(shape=(num_samples,max_len_targets,len(target_set)), dtype='float32')\n",
    "target_data = np.zeros((num_samples, max_len_targets, len(target_set)),dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_samples):\n",
    "    for k, note_value in enumerate(input_sequences[i]):\n",
    "        tokenized_input_sequences[i,k,input_note_to_index_dict[note_value]] = 1\n",
    "    \n",
    "    for k, note_value in enumerate(target_sequences[i]):\n",
    "        tokenized_target_sequences[i,k,target_note_to_index_dict[note_value]] = 1\n",
    "        \n",
    "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
    "        if k > 0:\n",
    "            target_data[i,k-1, target_note_to_index_dict[note_value]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "\n",
    "encoder_input = Input(shape=(None,len(input_set)))\n",
    "encoder_LSTM = Bidirectional(LSTM(256,return_state = True))\n",
    "encoder_outputs, encoder_h_forward, encoder_c_forward, encoder_h_back, encoder_c_back = encoder_LSTM (encoder_input)\n",
    "\n",
    "# Concatenate forward states and backward states\n",
    "encoder_h = Concatenate()([encoder_h_forward, encoder_h_back])\n",
    "encoder_c = Concatenate()([encoder_c_forward, encoder_c_back])\n",
    "#encoder_h = concatenate([encoder_h_forward, encoder_h_back])\n",
    "#encoder_c = concatenate([encoder_c_forward, encoder_c_back])\n",
    "encoder_states = [encoder_h, encoder_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"bidirectional_1/while/Exit_2:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"bidirectional_1/while_1/Exit_2:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"concatenate_1/concat:0\", shape=(?, 512), dtype=float32)\n",
      "Tensor(\"concatenate_2/concat:0\", shape=(?, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_h_forward)\n",
    "print(encoder_h_back)\n",
    "\n",
    "print(encoder_h)\n",
    "print(encoder_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder model\n",
    "\n",
    "decoder_input = Input(shape=(None,len(target_set)))\n",
    "\n",
    "# 512 because of concatenated forward states and backward states\n",
    "decoder_LSTM = LSTM(256*2,return_sequences=True, return_state = True)\n",
    "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(target_set),activation='softmax')\n",
    "decoder_out = decoder_dense (decoder_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38 samples, validate on 10 samples\n",
      "Epoch 1/10\n",
      "38/38 [==============================] - 5s 123ms/step - loss: 3.3342 - val_loss: 3.1467\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 3.1419 - val_loss: 2.9317\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 2.9111 - val_loss: 2.4895\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 2.4369 - val_loss: 2.1871\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 1.9974 - val_loss: 1.7700\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 1.6060 - val_loss: 1.7668\n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 1.6470 - val_loss: 1.5970\n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 1.4536 - val_loss: 1.4880\n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 1.2931 - val_loss: 1.5544\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 2s 61ms/step - loss: 1.3140 - val_loss: 1.5766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/torgrim/anaconda3/lib/python3.5/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_1/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.fit(x=[tokenized_input_sequences,tokenized_target_sequences], \n",
    "          y=target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "model.save(\"trained_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference models for testing\n",
    "\n",
    "# Encoder inference model\n",
    "encoder_model_inf = Model(encoder_input, encoder_states)\n",
    "\n",
    "# Decoder inference model\n",
    "decoder_state_input_h = Input(shape=(256*2,))\n",
    "decoder_state_input_c = Input(shape=(256*2,))\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
    "                                                 initial_state=decoder_input_states)\n",
    "\n",
    "decoder_states = [decoder_h , decoder_c]\n",
    "\n",
    "decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
    "                          outputs=[decoder_out] + decoder_states )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(inp_seq):\n",
    "    \n",
    "    # Initial states value is coming from the encoder \n",
    "    states_val = encoder_model_inf.predict(inp_seq)\n",
    "   \n",
    "    target_seq = np.zeros((1, 1, len(target_set)))\n",
    "    target_seq[0, 0, target_note_to_index_dict[200]] = 1\n",
    "\n",
    "    decoded_sequence = []\n",
    "    stop_condition = False\n",
    "    \n",
    "    while not stop_condition:       \n",
    "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)      \n",
    "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "        sampled_note = target_index_to_note_dict[max_val_index]\n",
    "        decoded_sequence.append(sampled_note)\n",
    "        \n",
    "        if ((sampled_note == 201) or (len(decoded_sequence) > max_len_targets)):\n",
    "            stop_condition = True\n",
    "        \n",
    "        target_seq = np.zeros((1, 1, len(target_set)))\n",
    "        target_seq[0, 0, max_val_index] = 1\n",
    "        \n",
    "        states_val = [decoder_h, decoder_c]\n",
    "        \n",
    "    return np.array(decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload test data\n",
    "\n",
    "from google.colab import files\n",
    "uploaded_file = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('test_data.npz') as data:\n",
    "    test_data = data['train']\n",
    "# Only if using the inputs from training set and some notes are missing\n",
    "#test_data = test_data[0:len(test_data):2]\n",
    "\n",
    "max_len_test_data = max([len(seq) for seq in test_data])\n",
    "tokenized_inputs = np.zeros(shape=(len(test_data),max_len_test_data,len(input_set)), dtype='float32')  \n",
    "    \n",
    "for i in range(len(test_data)):\n",
    "    for k, note_value in enumerate(test_data[i]):\n",
    "        tokenized_inputs[i,k,input_note_to_index_dict[note_value]] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all samples as file.\n",
    "# Input sequences on even index and decoded sequences on odd index\n",
    "samples = []\n",
    "for seq_index in range(len(test_data)):\n",
    "    inp_seq = tokenized_inputs[seq_index:seq_index+1]\n",
    "    decoded_sequence = decode_seq(inp_seq)\n",
    "    samples.append(test_data[seq_index])\n",
    "    samples.append(decoded_sequence)\n",
    "samples = np.array(samples)\n",
    "np.savez('sample_pairs.npz', samples=samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download test pairs\n",
    "files.download('sample_pairs.npz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
