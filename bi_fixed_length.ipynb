{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/kmsravindra/ML-AI-experiments/blob/master/AI/Neural%20Machine%20Translation/Neural%20machine%20translation%20-%20Encoder-Decoder%20seq2seq%20model.ipynb\n",
    "import os\n",
    "\"\"\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\"\"\"\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "from time import gmtime, strftime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "epochs=1\n",
    "train_path = \"training_sets_split/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  trained_models/13.13.18_11:12_bidirectional  Created \n",
      "Directory  trained_models/13.13.18_11:12_bidirectional/weights  Created \n",
      "Directory  trained_models/13.13.18_11:12_bidirectional/samples  Created \n",
      "Directory  trained_models/13.13.18_11:12_bidirectional/history  Created \n"
     ]
    }
   ],
   "source": [
    "network = \"bidirectional\"\n",
    "datatype = \"normal, transposed to A minor and C major\"\n",
    "stride = 4\n",
    "encoder_dropout = True\n",
    "decoder_dropout = False\n",
    "dropout_encoder_amount = 0.2\n",
    "dropout_decoder_amount = 0.2\n",
    "\n",
    "timestamp = strftime(\"%d.%d.%y_%H:%M\", gmtime())\n",
    "folder_name = \"trained_models/\"+timestamp+\"_\"+network\n",
    "weights_folder_name = folder_name + \"/weights\"\n",
    "samples_folder_name = folder_name + \"/samples\"\n",
    "history_folder_name = folder_name + \"/history\"\n",
    " \n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir(folder_name)\n",
    "    print(\"Directory \" , folder_name ,  \" Created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , folder_name ,  \" already exists\")\n",
    "    \n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir(weights_folder_name)\n",
    "    print(\"Directory \" , weights_folder_name ,  \" Created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , weights_folder_name ,  \" already exists\")\n",
    "    \n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir(samples_folder_name)\n",
    "    print(\"Directory \" , samples_folder_name ,  \" Created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , samples_folder_name ,  \" already exists\")\n",
    "\n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir(history_folder_name)\n",
    "    print(\"Directory \" , history_folder_name ,  \" Created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , history_folder_name ,  \" already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training inputs 8962\n",
      "Number of training targets 8962\n",
      "Input example: [ 60 129 129 129  67 129 129 129  64 129 129 129  60 129 129 129  67 129\n",
      " 129 129  69 129 129 129  69 129 129 129  67 129 129 129]\n",
      "Target example: [ 55 129 129 129  55 129 129 129  55 129 129 129  57 129 129 129  60 129\n",
      " 129 129  65 129 129 129  65 129 129 129  64 129 129 129]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and create pairs of inputs and targets\n",
    "\n",
    "with np.load(train_path+'split_inputs.npz') as split_inputs:\n",
    "    inputs = split_inputs['train']\n",
    "    \n",
    "with np.load(train_path+'split_targets.npz') as split_targets:\n",
    "    targets = split_targets['train']\n",
    "\n",
    "print(\"Number of training inputs\",len(inputs))\n",
    "print(\"Number of training targets\",len(targets))\n",
    "\n",
    "# Print examples of inputs and targets\n",
    "example_in = inputs[0]\n",
    "example_target = targets[0]\n",
    "print(\"Input example:\", example_in)\n",
    "print(\"Target example:\", example_target)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set\n",
    "with np.load(train_path+'split_test_inputs.npz') as test_inputs:\n",
    "    test_soprano_tenor = test_inputs['train']\n",
    "with np.load(train_path+'split_test_targets.npz') as test_targets:\n",
    "    test_alto_bass = test_targets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make set of unique notes\n",
    "split_inputs_uniq = np.unique(inputs)\n",
    "split_targets_uniq = np.unique(targets)\n",
    "test_soprano_tenor_uniq = np.unique(test_soprano_tenor)\n",
    "test_alto_bass_uniq = np.unique(test_alto_bass)\n",
    "input_set = set()\n",
    "target_set = set()\n",
    "for note in split_inputs_uniq:\n",
    "    input_set.add(note)\n",
    "    target_set.add(note)\n",
    "for note in split_targets_uniq:\n",
    "    input_set.add(note)\n",
    "    target_set.add(note)\n",
    "for note in test_soprano_tenor_uniq:\n",
    "    input_set.add(note)\n",
    "    target_set.add(note)\n",
    "for note in test_alto_bass_uniq:\n",
    "    input_set.add(note)\n",
    "    target_set.add(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set of input notes:\n",
      " [31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 128, 129]\n",
      "Set of target notes:\n",
      " [31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 128, 129]\n"
     ]
    }
   ],
   "source": [
    "input_sequences = list(inputs)\n",
    "target_sequences = []\n",
    "num_samples = len(inputs)\n",
    "    \n",
    "for i in range(num_samples):    \n",
    "    target_with_tokens = list(targets[i])\n",
    "    target_sequences.append(target_with_tokens)\n",
    "\n",
    "            \n",
    "input_set = sorted(list(input_set))\n",
    "target_set = sorted(list(target_set))\n",
    "print(\"Set of input notes:\\n\", input_set)\n",
    "print(\"Set of target notes:\\n\", target_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to index each input note - key is index and value is the note\n",
    "input_index_to_note_dict = {}\n",
    "\n",
    "# dictionary to get note given its index - key is the note and value is the index\n",
    "input_note_to_index_dict = {}\n",
    "\n",
    "for k, v in enumerate(input_set):\n",
    "    input_index_to_note_dict[k] = v\n",
    "    input_note_to_index_dict[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to index each target note - key is index and value is the note\n",
    "target_index_to_note_dict = {}\n",
    "\n",
    "# dictionary to get note given its index - key is the note and value is the index\n",
    "target_note_to_index_dict = {}\n",
    "\n",
    "for k, v in enumerate(target_set):\n",
    "    target_index_to_note_dict[k] = v\n",
    "    target_note_to_index_dict[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "max_len_inputs = max([len(seq) for seq in input_sequences])\n",
    "max_len_targets = max([len(line) for line in target_sequences])\n",
    "print(max_len_inputs)\n",
    "print(max_len_targets)\n",
    "\n",
    "doc = []\n",
    "doc.append(\"Network type:\\t\\t\\t\"+network)\n",
    "doc.append(\"Dataset type:\\t\\t\\t\"+datatype)\n",
    "\n",
    "if encoder_dropout:\n",
    "    doc.append(\"Encoder dropout:\\t\\t{}\".format(dropout_encoder_amount))\n",
    "if decoder_dropout:\n",
    "    doc.append(\"Decoder dropout:\\t\\t{}\".format(dropout_decoder_amount))\n",
    "\n",
    "doc.append(\"Batch size:\\t\\t\\t{}\".format(batch_size))\n",
    "doc.append(\"Sample length:\\t\\t\\t{}\".format(max_len_inputs))\n",
    "doc.append(\"Stride:\\t\\t\\t\\t{}\".format(stride))\n",
    "doc.append(\"Number of training inputs:\\t{}\".format(len(inputs)))\n",
    "doc.append(\"Number of training targets:\\t{}\".format(len(targets)))\n",
    "noteset = ' '.join(str(value) for value in input_set)\n",
    "doc.append(\"Set of notes:\\n \" + noteset)\n",
    "\n",
    "\n",
    "file = open(folder_name + \"/Specifications.txt\", \"w\")\n",
    "file.write(\"\\n\".join(doc))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input_sequences = np.zeros(shape=(num_samples,max_len_inputs,len(input_set)), dtype='float32')\n",
    "tokenized_target_sequences = np.zeros(shape=(num_samples,max_len_targets,len(target_set)), dtype='float32')\n",
    "target_data = np.zeros((num_samples, max_len_targets, len(target_set)),dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_samples):\n",
    "    for k, note_value in enumerate(input_sequences[i]):\n",
    "        tokenized_input_sequences[i,k,input_note_to_index_dict[note_value]] = 1\n",
    "    \n",
    "    for k, note_value in enumerate(target_sequences[i]):\n",
    "        tokenized_target_sequences[i,k,target_note_to_index_dict[note_value]] = 1\n",
    "        \n",
    "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
    "        if k > 0:\n",
    "            target_data[i,k-1, target_note_to_index_dict[note_value]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "\n",
    "encoder_input = Input(shape=(None,len(input_set)))\n",
    "\n",
    "if encoder_dropout:\n",
    "    encoder_LSTM = Bidirectional(LSTM(256,return_state = True, recurrent_dropout=dropout_encoder_amount))\n",
    "else:\n",
    "    encoder_LSTM = Bidirectional(LSTM(256,return_state = True))\n",
    "    \n",
    "encoder_outputs, encoder_h_forward, encoder_c_forward, encoder_h_back, encoder_c_back = encoder_LSTM (encoder_input)\n",
    "\n",
    "# Concatenate forward states and backward states\n",
    "encoder_h = Concatenate()([encoder_h_forward, encoder_h_back])\n",
    "encoder_c = Concatenate()([encoder_c_forward, encoder_c_back])\n",
    "encoder_states = [encoder_h, encoder_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"bidirectional_1/while/Exit_2:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"bidirectional_1/while_1/Exit_2:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"concatenate_1/concat:0\", shape=(?, 512), dtype=float32)\n",
      "Tensor(\"concatenate_2/concat:0\", shape=(?, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_h_forward)\n",
    "print(encoder_h_back)\n",
    "\n",
    "print(encoder_h)\n",
    "print(encoder_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder model\n",
    "\n",
    "decoder_input = Input(shape=(None,len(target_set)))\n",
    "\n",
    "# 512 because of concatenated forward states and backward states\n",
    "if decoder_dropout:\n",
    "    decoder_LSTM = LSTM(256*2,return_sequences=True, return_state = True, recurrent_dropout=dropout_decoder_amount)\n",
    "else:\n",
    "    decoder_LSTM = LSTM(256*2,return_sequences=True, return_state = True)\n",
    "    \n",
    "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(target_set),activation='softmax')\n",
    "decoder_out = decoder_dense (decoder_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=weights_folder_name+\"/{epoch:02d}-loss_{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, mode='min', save_best_only=False)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7169 samples, validate on 1793 samples\n",
      "Epoch 1/1\n",
      "7169/7169 [==============================] - 112s 16ms/step - loss: 1.5792 - val_loss: 1.2825\n",
      "\n",
      "Epoch 00001: saving model to trained_models/13.13.18_11:12_bidirectional/weights/01-loss_1.28.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/torgrim/anaconda3/lib/python3.5/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_1/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "history = model.fit(x=[tokenized_input_sequences,tokenized_target_sequences], \n",
    "          y=target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2,callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save loss and validation loss\n",
    "loss = np.array(history.history['loss'])\n",
    "validation_loss = np.array(history.history['val_loss'])\n",
    "loss_name = history_folder_name+'/loss.npz'\n",
    "val_loss_name = history_folder_name+'/validation_loss.npz'\n",
    "np.savez(loss_name, loss=loss)\n",
    "np.savez(val_loss_name, loss=validation_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with np.load(loss_name) as data:\n",
    "    loss = list(data['loss'])\n",
    "    \n",
    "with np.load(val_loss_name) as data:\n",
    "    val_loss = list(data['loss'])\n",
    "\n",
    "epoch = np.arange(epochs)\n",
    "\n",
    "plt.plot(epoch, loss, label=\"Training loss\", color=\"blue\", linewidth=1, linestyle=\"-\")    # Add labels for the legend\n",
    "plt.plot(epoch, val_loss, label=\"Validation loss\", color=\"red\", linewidth=1, linestyle=\"-\")    # Add labels for the legend\n",
    "plt.xlabel('Epoch')   # Add labels for the axis\n",
    "\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best weights\n",
    "# model.load_weights(weights_folder_name+\"/best_epoch_06-loss_1.59.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference models for testing\n",
    "\n",
    "# Encoder inference model\n",
    "encoder_model_inf = Model(encoder_input, encoder_states)\n",
    "\n",
    "# Decoder inference model\n",
    "decoder_state_input_h = Input(shape=(256*2,))\n",
    "decoder_state_input_c = Input(shape=(256*2,))\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
    "                                                 initial_state=decoder_input_states)\n",
    "\n",
    "decoder_states = [decoder_h , decoder_c]\n",
    "\n",
    "decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
    "                          outputs=[decoder_out] + decoder_states )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(inp_seq, input_length):\n",
    "    \n",
    "    # Initial states value is coming from the encoder \n",
    "    states_val = encoder_model_inf.predict(inp_seq)\n",
    "   \n",
    "    target_seq = np.zeros((1, 1, len(target_set)))\n",
    "\n",
    "    decoded_sequence = []\n",
    "    stop_condition = False\n",
    "    input_seq_length = len(inp_seq)\n",
    "    \n",
    "    while not stop_condition:       \n",
    "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)      \n",
    "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "        sampled_note = target_index_to_note_dict[max_val_index]\n",
    "        decoded_sequence.append(sampled_note)\n",
    "        \n",
    "        if len(decoded_sequence) == input_length:\n",
    "            stop_condition = True\n",
    "        \n",
    "        target_seq = np.zeros((1, 1, len(target_set)))\n",
    "        target_seq[0, 0, max_val_index] = 1\n",
    "        \n",
    "        states_val = [decoder_h, decoder_c]   \n",
    "    return np.array(decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "# Sample with soprano and tenor from test set\n",
    "\n",
    "with np.load(train_path+'test_inputs.npz') as test_targets:\n",
    "    test_soprano_tenor = test_targets['train']\n",
    "    \n",
    "max_len_test_soprano_tenor = max([len(seq) for seq in test_soprano_tenor])\n",
    "tokenized_inputs_sop_ten = np.zeros(shape=(len(test_soprano_tenor),max_len_test_soprano_tenor,len(input_set)), dtype='float32')  \n",
    "    \n",
    "for i in range(len(test_soprano_tenor)):\n",
    "    for k, note_value in enumerate(test_soprano_tenor[i]):\n",
    "        tokenized_inputs_sop_ten[i,k,input_note_to_index_dict[note_value]] = 1 \n",
    "\n",
    "\n",
    "# Save all samples as file.\n",
    "# Input sequences on even index and decoded sequences on odd index\n",
    "\n",
    "samples = []\n",
    "for seq_index, seq in enumerate(test_soprano_tenor):\n",
    "    if seq_index % 10 == 0:\n",
    "        print(seq_index)\n",
    "    inp_seq = tokenized_inputs_sop_ten[seq_index:seq_index+1]\n",
    "    input_seq_length = len(seq)\n",
    "    decoded_sequence = decode_seq(inp_seq, input_seq_length)\n",
    "    samples.append(test_soprano_tenor[seq_index])\n",
    "    samples.append(decoded_sequence)\n",
    "samples = np.array(samples)\n",
    "np.savez(samples_folder_name+'/sampled_soprano_tenor.npz', samples=samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "# Sample with alto and bass from test set\n",
    "\n",
    "with np.load(train_path+'test_targets.npz') as test_targets:\n",
    "    test_alto_bass = test_targets['train']\n",
    "\n",
    "max_len_test_alto_bass = max([len(seq) for seq in test_alto_bass])\n",
    "tokenized_inputs_alt_bas = np.zeros(shape=(len(test_alto_bass),max_len_test_alto_bass,len(input_set)), dtype='float32')  \n",
    "    \n",
    "for i in range(len(test_soprano_tenor)):\n",
    "    for k, note_value in enumerate(test_alto_bass[i]):\n",
    "        tokenized_inputs_alt_bas[i,k,input_note_to_index_dict[note_value]] = 1 \n",
    "\n",
    "\n",
    "# Save all samples as file.\n",
    "# Input sequences on even index and decoded sequences on odd index\n",
    "\n",
    "samples = []\n",
    "for seq_index, seq in enumerate(test_alto_bass):\n",
    "    if seq_index % 10 == 0:\n",
    "        print(seq_index)\n",
    "    inp_seq = tokenized_inputs_alt_bas[seq_index:seq_index+1]\n",
    "    input_seq_length = len(seq)\n",
    "    decoded_sequence = decode_seq(inp_seq, input_seq_length)\n",
    "    samples.append(test_alto_bass[seq_index])\n",
    "    samples.append(decoded_sequence)\n",
    "samples = np.array(samples)\n",
    "np.savez(samples_folder_name+'/sampled_alto_bass.npz', samples=samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
